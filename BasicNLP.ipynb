{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9224467d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yiweihan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yiweihan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yiweihan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yiweihan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.a\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f54c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/yiweihan/Desktop/corona_fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7829b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.b.i\n",
    "data = data.dropna(axis = 0, how ='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba80c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index = range(len(data))\n",
    "tokens = []\n",
    "for i in range (data.shape[0]):\n",
    "    temp = nltk.word_tokenize(data['text'][i])\n",
    "    tokens.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efa77d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.b.ii\n",
    "tagged =[]\n",
    "for i in tokens:\n",
    "    temp = nltk.pos_tag(i)\n",
    "    tagged.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6928a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.b.iii\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatize = []\n",
    "for i in tagged:\n",
    "    lemmas_sent= []\n",
    "    for tag in i:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmas_sent.append(wnl.lemmatize(tag[0], pos = wordnet_pos))\n",
    "    lemmatize.append(lemmas_sent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f40d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.b.iv\n",
    "from nltk.corpus import stopwords\n",
    "filtered_words = []\n",
    "for i in lemmatize:\n",
    "    temp = [word for word in i if word not in stopwords.words('english')]\n",
    "    filtered_words.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af73f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.b.v\n",
    "import re\n",
    " \n",
    "def remove(text):\n",
    "    remove_chars = '[0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'\n",
    "    return re.sub(remove_chars, '', text)\n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", sample)\n",
    "\n",
    "text_clean = []\n",
    "for filtered_word in filtered_words:\n",
    "    for i in filtered_word:\n",
    "        if len(i)<3:\n",
    "            filtered_word.remove(i)\n",
    "    temp1 = remove(\" \".join(i for i in filtered_word))\n",
    "    temp2 = remove_URL(\"\".join(i for i in temp1))\n",
    "    text_clean.append(temp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e80d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.a\n",
    "# n-diagram is a n-item sequence, which can be phonemes, syllables, letters, words or base pairs\n",
    "#from a given sample of text and speech. Based on some sample or storage of language, people can use\n",
    "#this to predict or evaluate whether a sentence is reasonable. The other application can be to evaluate\n",
    "#the difference between two texts. The contents of the text are operated by a sliding window with a size\n",
    "#of N in bytes, forming a sequence of byte fragments with a length of N. In order to facilitate calculation,\n",
    "#we only consider the front n-1 words in n-digram. N-gram’s advantage is that it is widely used to compress\n",
    "#text, check for spelling errors, speed up string lookup, and document language recognition. And it is\n",
    "#the most useful of automatic classification of natural languages as people can not process those huge amount\n",
    "#of items by their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63281ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.b.\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c60bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.c.\n",
    "data['clean_text'] = text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb0d4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"clean_text\"]\n",
    "Y = data[\"label\"]\n",
    "countvectorizer1 = CountVectorizer(lowercase=True, ngram_range=(1,1))\n",
    "countvectorizer2 = CountVectorizer(lowercase=True, ngram_range=(1,2))\n",
    "countvectorizer3 = CountVectorizer(lowercase=True, ngram_range=(1,3))\n",
    "X_countvectorizer1 = countvectorizer1.fit_transform(X)\n",
    "X_countvectorizer2 = countvectorizer2.fit_transform(X)\n",
    "X_countvectorizer3 = countvectorizer3.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d0b64a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.d.\n",
    "X = data[\"clean_text\"]\n",
    "Y = data[\"label\"]\n",
    "tfidfvectorizer1 = TfidfVectorizer(lowercase=True, ngram_range =(1,1))\n",
    "tfidfvectorizer2 = TfidfVectorizer(lowercase=True, ngram_range=(1,2))\n",
    "tfidfvectorizer3 = TfidfVectorizer(lowercase=True, ngram_range=(1,3))\n",
    "X_tfidfvectorizer1 = tfidfvectorizer1.fit_transform(X)\n",
    "X_tfidfvectorizer2 = tfidfvectorizer2.fit_transform(X)\n",
    "X_tfidfvectorizer3 = tfidfvectorizer3.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d3bebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.a.\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "CV = LogisticRegressionCV(cv=5, max_iter=1000, n_jobs = -1, random_state=265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9b0cd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9150943396226415"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_countvectorizer1, Y, test_size = 0.3, random_state = 265)\n",
    "\n",
    "\n",
    "c1 = CV.fit(X_train, y_train)\n",
    "c1_predict = CV.predict(X_test)\n",
    "C1_Accuracy = c1.score(X_test,y_test)\n",
    "C1_Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d823e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9276729559748428"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_countvectorizer2, Y, test_size = 0.3, random_state = 265)\n",
    "\n",
    "\n",
    "c2 = CV.fit(X_train, y_train)\n",
    "c2_predict = CV.predict(X_test)\n",
    "C2_Accuracy = c2.score(X_test,y_test)\n",
    "C2_Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3779ddba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9119496855345912"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_countvectorizer3, Y, test_size = 0.3, random_state = 265)\n",
    "\n",
    "\n",
    "c3 = CV.fit(X_train, y_train)\n",
    "c3_predict = CV.predict(X_test)\n",
    "C3_Accuracy = c3.score(X_test,y_test)\n",
    "C3_Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfde8414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.940251572327044"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.b.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidfvectorizer1, Y, test_size = 0.3, random_state = 265)\n",
    "t1 = CV.fit(X_train, y_train)\n",
    "t1_predict = CV.predict(X_test)\n",
    "T1_Accuracy = t1.score(X_test,y_test)\n",
    "T1_Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14879423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9371069182389937"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidfvectorizer2, Y, test_size = 0.3, random_state = 265)\n",
    "\n",
    "\n",
    "t2 = CV.fit(X_train, y_train)\n",
    "t2_predict = CV.predict(X_test)\n",
    "T2_Accuracy = t2.score(X_test,y_test)\n",
    "T2_Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88c60b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9308176100628931"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidfvectorizer3, Y, test_size = 0.3, random_state = 265)\n",
    "\n",
    "\n",
    "t3 = CV.fit(X_train, y_train)\n",
    "t3_predict = CV.predict(X_test)\n",
    "T3_Accuracy = t3.score(X_test,y_test)\n",
    "T3_Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c90ba3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CV_11</td>\n",
       "      <td>0.915094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CV_12</td>\n",
       "      <td>0.927673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CV_13</td>\n",
       "      <td>0.911950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TV_11</td>\n",
       "      <td>0.940252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TV_12</td>\n",
       "      <td>0.937107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TV_13</td>\n",
       "      <td>0.930818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type  accuracy\n",
       "0  CV_11  0.915094\n",
       "1  CV_12  0.927673\n",
       "2  CV_13  0.911950\n",
       "3  TV_11  0.940252\n",
       "4  TV_12  0.937107\n",
       "5  TV_13  0.930818"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.c.\n",
    "result = pd.DataFrame()\n",
    "result['type'] = ['CV_11','CV_12','CV_13','TV_11','TV_12','TV_13']\n",
    "result['accuracy'] = [C1_Accuracy, C2_Accuracy, C3_Accuracy, T1_Accuracy, T2_Accuracy, T3_Accuracy]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0632245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.a\n",
    "#The traditional Newton method requires the inverse of The Hessian matrix every iteration,\n",
    "#which is very complicated. In order to avoid the inverse of the matrix, Newton-CG uses CG conjugate\n",
    "#gradient method to solve the linear equations, thus avoiding the inverse of the matrix.\n",
    "#Newton-cg iteratively optimizes the loss function by using the hessian matrix, the second derivative\n",
    "#matrix of the loss function. Therefore, it cannot be used for L1 regularization without continuous derivatives,\n",
    "#but only for L2 regularization. And it is often used as we have a large sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.b\n",
    "#It also, uses the second derivative matrix (Hessian Matrix) of the loss function to optimize the loss\n",
    "#function iteratively. In BFGS algorithm, each iteration requires the Hesse matrix obtained by the previous\n",
    "#iteration. The storage space of the matrix is at least N(N+1)/2, and N is the characteristic dimension.\n",
    "#For high-dimensional application scenarios, the storage space required will be very large. The basic idea\n",
    "#of L-BFGS is to replace the previous Hesse matrix by storing a small amount of data from the previous\n",
    "#m iterations. L-bfgs algorithm can be understood as another approximation of BFGS algorithm, that reduce storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.c\n",
    "#LIBLINEAR is a linear classifier for data with millions of instances and features.\n",
    "#Liblinear is implemented by the open source Liblinear library, and the internal use of the axis descent method\n",
    "#to iteratively optimize the loss function. It supports L2-regularized logistic regression (LR), L2-loss and L1-loss\n",
    "#linear support vector machines. It has advantages like simple usage, rich documentation, and open source license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3be58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.d\n",
    "#Sag, also known as stochastic mean gradient descent, is a variant of gradient descent method.\n",
    "#The difference between sag and ordinary gradient descent method is that only a part of samples are used\n",
    "#in each iteration to calculate the gradient, which is suitable for a large number of sample data.\n",
    "#In each calculation, the value of two gradients is used, one is the gradient value of the previous iteration\n",
    "#and the other is the new gradient value. The SAG algorithm maintains an old gradient YI in memory for each\n",
    "#sample, randomly selects a sample I to update D, and uses D to update parameter X. Specifically, the updated\n",
    "#term d comes from replacing the old gradient y_i in d with a new gradient fi'(x), which is what d = d-yi + fi'(x)\n",
    "#means. Thus, each update only computes the gradient of one sample, not the gradient of all samples.\n",
    "#The computational overhead is the same as SGD, but the memory overhead is much higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c11f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.e\n",
    "#The SAGA algorithm is an accelerated version of the SAG algorithm. Like SAG, it neither operates within a\n",
    "#loop nor calculates batch gradients (except at initial points). In each iteration, it calculates random vectors\n",
    "#as the average of random gradients from previous iterations. In particular, for k iterations, the algorithm will\n",
    "#store [formula], for all previous [formulas], where [formula] represents the most recent iteration computed by\n",
    "#[formula]. The integers [formula] are chosen arbitrarily."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
